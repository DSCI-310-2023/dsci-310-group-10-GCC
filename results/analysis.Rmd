---
title: 'Predicting Forest Fires in Algeria'
author: "Rocky Zhao, Jinghan Xu, and Jan Zimny"
output:
  bookdown::html_document2:
    toc: true
  bookdown::pdf_document2:
    toc: true
    latex_engine: xelatex
---

```{r, include=FALSE}
library(here)
library(tidyverse)
library(tidymodels)
```

Forest fires are a type of uncontrolled and unwanted fire that usually have a negative impact. In 2007, forest fires in the Atlas Mountains, located on the northern coast of Algeria, killed several people as it spread rapidly due to hot, dry winds  [3].
In this project, the main question which we want to answer is:

**Can we predict forest fires given the weather conditions by using k nearest neighbours?**

Our dataset contains weather information on the Sidi Bel Abb√®s and the Bejaia region, and includes information such as:

- `day`, `month`, `year`: the date of the instance

- `Temperature`: the maximum temperature in degrees celsius

- `RH`: Relative humidity in percentages

- `Ws`: wind speed in kilometers per hour

- `Rain`: total amount of rain, in mm

- `Classes`: whether or not there was fire

The data set also includes several components from [Canadian Forest Fire Weather Index (FWI) System](https://cwfis.cfs.nrcan.gc.ca/background/summary/fwi) Components:

- `FFMC`: Fine Fuel Moisture Code, measures the moisture content of litter and other cured fine fuels.

- `DMC`: Dull Moisture Code, measures the average moisture content of loosely compacted organic layers of moderate depth.

- `DC`: Drought Code, measures the average moisture content of deep, compact organic layers.

- `ISI`: Initial Spread Index, measures the expected rate of fire spread. It is based on wind speed and FFMC.

- `BUI`: Buildup Index, measures the total amount of fuel available for combustion.

- `FWI`: Fire Weather Index, measures fire intensity.


# Loading and wrangling the dataset
The first thing we do is to look at the dataset online, to see what it looks like. At first glance, we can see that this dataset contains headings, and uses commas as a delimiter. Moreover, it seems like each region is stored in a "separate" table on the same dataset. This means that we will need to do some cleaning after we load it to our notebook.
After inspecting the dataset, we take the data set's url from [this website](https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset++) and load it to our notebook. The dataset has one table for each region, so based on the loaded data we create 2 datasets.

```{r forest-fire-head, echo=FALSE, message=FALSE}
forest_fires <- read_csv(here("results/analysis_data/forest_fires.csv"))
knitr::kable(forest_fires[1:10, ],
  caption =
    "First 10 entries of the forest fire dataset"
)
```

# Exploring the dataset 

Usually, we do the preliminary exploratory data analysis with only the training set, so we need to split the `forest_fires` data set into a training and testing set first. Because our dataset is quite small, we decided to have more observations in the training set.|

```{r, echo = FALSE, message = FALSE}
fire_train <- read_csv(here("results/analysis_data/fire_train.csv"))
```

The training set takes up `r nrow(fire_train)/nrow(forest_fires)` percent of the total data.

After making the training set, now we can start exploring the data and decide what predictors to use. Because everything was recorded in 2012, so we don't want to use `day`, `month` and `year`. We removed these columns right before we split the data into the training and testing. Next, we want to try and summarize the data in different ways:

```{r entries-by-class, echo=FALSE, message=FALSE}
# this tells us how many entries are on each factor in Classes

fire_training_count <- read_csv(here("results/analysis_data/fire_training_count.csv"))
knitr::kable(fire_training_count, caption = "Entries by class")
```

From this, we know that there is a pretty equal distribution of observations between the two classes. This is a good thing, because it means we don't need any additional steps such as repeating the observations from the class with less observations.

```{r fire-training-range-fire-head, echo=FALSE, message=FALSE}
fire_training_range_fire <- read_csv(here("results/analysis_data/fire_training_range_fire.csv"))
knitr::kable(fire_training_range_fire, caption = "Value ranges of data samples which have the class 'fire'")
```

```{r fire-training-range-no-fire-head, echo=FALSE, message=FALSE}
fire_training_range_not_fire <- read_csv(here("results/analysis_data/fire_training_range_not_fire.csv"))
knitr::kable(fire_training_range_not_fire, caption = "Value ranges of data samples which have the class 'not fire'")
```

```{r fire-training-mean-fire-head, echo=FALSE, message=FALSE}
fire_training_mean_fire <- read_csv(here("results/analysis_data/fire_training_mean_fire.csv"))
knitr::kable(fire_training_mean_fire, caption = "Averages of data samples which have the class 'fire'")
```

```{r fire-training-mean-no-fire-head, echo=FALSE, message=FALSE}
fire_training_mean_not_fire <- read_csv(here("results/analysis_data/fire_training_mean_not_fire.csv"))
knitr::kable(fire_training_mean_not_fire, caption = "Averages of data samples which have the class 'not fire'")
```

We can see from table 2.2 and 2.3 that the ranges of values for Ws are really similar, which are 8-21 and 6-29. 
And from table 2.4 and 2.5, we can see that their averages are also really similar, which are 15.41 and 15.72.
This means that there isn't a clear distinction between the range of values from different Classes, and this also means that their averages are too similar. We don't want to use columns like this, because it's probably not going to be useful in differentiating the Classes. Another column we might not want to use isRH.
Because it's easier to process visual information, we draw some plots:

```{r correlation-plot, echo=FALSE, out.width="100%", fig.cap = "Correlations between each of the variables in the dataset"}
knitr::include_graphics(here("results/analysis_data/correlation_graph.png"))
```

As we can see from this plot, our previous conclusion that values like Ws won't be a good choice can be seen by how both rain and not rain Ws values overlap - we can't "group" the values into groups of rain/ not rain.
Based on prior research on `FWI` indexes [1], we know that they're all calculated based on temperature, relative humidity, wind and rain, which are the `Temperature`, `RH`, `Ws`, and `Rain` columns. Additionally, We also know that `ISI` is calculated from `Ws` and `FFMC`, and `BUI` is calculated from `DMC` and `DC`. These 2 columns encompass information from other columns, so it seems reasonable that we use them to predict `Classes`.
To confirm our choice, we make a plot with `ISI`, `BUI` and `Classes` to see if this choice makes sense based on the actual data itself.


```{r scatter-plot, echo=FALSE, out.width="70%", fig.align="center", fig.cap = "Scatter plot for variables ISI and BUI"}
knitr::include_graphics(here("results/analysis_data/scatter_plot.png"))
```

There is a positive correlation between `ISI` and `BUI`, and there seems to be a distinction between the Classes! So the columns `ISI` and `BUI` seem like a reasonable choice. Moreover, because we are only using 2 predictors, it'll be easier to make a visualization of our results/analysis_data.

Now that we know which predictors to use, we can start making our model.


# Methodology

This is a classification problem and we use the K-Nearest Neighbor model as we want to classify whether there will be forest fire in 2012 by predict predicting the value if `Classes`. The predictors we choose are  `ISI`, Initial Spread Index, and `BUI`, Buildup Index.

The general rule of thumb of picking a split ratio is the larger the data set, the closer to a 50/50 split ratio. Since we have a relatively small amount of data (less than 250 instances), we will split 75% for the training set, and 25% for the testing set, also following one of the most common split ratios.

Before we can make the model, we need to choose a _k_ value that works. So, we need to tune the classifier. 
The steps are as follows:

1. We make a 10-fold cross validation, to generate the testing and validation sets. Because our dataset is relatively small, we decided to use 10-fold instead of 5 fold to compensate for the fewer observations.

2. Make the recipe, using Classes as the target variable and `ISI` and `BUI` as the predictor variables, using data from the training set.

3. Then, we make a model specification with the `nearest_neighbor` function, setting the neighbors to `tune()`.

4. We add this recipe and model to a `workflow()`, using `collect_metrics()` to get the accuracies of each _k_ value.

# Results

```{r accuracies-head, echo=FALSE, message=FALSE}
accuracies <- read_csv(here("results/analysis_data/accuracies.csv"))
knitr::kable(accuracies[1:10, ], caption = "Accuracies per number of neighbors respected, first ten rows of data")
```

Now, we know how accurate our model is for different _k_ values but since we have 50 values, it's hard to compare all 50 at once! Again, We plot the accuracies so that we can compare the accuracies between each _k_ value in a more efficient and better way.

```{r line-plot, echo=FALSE, out.width="70%", fig.align="center", fig.cap = "Line plot visualizing the accuracy per number of neighbors taken into account while classifying"}
knitr::include_graphics(here("results/analysis_data/line_plot.png"))
```

From this graph, we can see the it peaks at about 5/6 and then starts to reduce as _k_ increases. K-nearest model gets slower with larger _k_ values, so we decided that _k_ = 5 would be the most ideal choice for our model, because of its high accuracy estimate and since this estimate doesn't change much when compared to nearby _k_ values.
This graph also makes sense because we know that small _k_ values tend to have lower accuracies due to underfitting, while large _k_ values will also have lower accuracies due to overfitting. In this graph, accuracies initially increase, and then generally decreases as _k_ values increase.
Now that we know what _k_ value to use, we make a new model specification, this time setting _k_ to 5. We add this new model and the recipe to a workflow.

Now, that we have a model, we can use it to predict the classes on our testing set!


```{r fire-pred-head, echo=FALSE, message=FALSE}
fire_test_predictions <- read_csv(here("results/analysis_data/fire_test_predictions.csv"))
knitr::kable(fire_test_predictions[1:10, ], caption = "Peak into the entries which have been classified, first ten rows of data")
```

To get a better picture on how well our model predicted the testing set's classes, we can get the metrics and the confusion matrix:

```{r, echo=FALSE}
# We kept this code in the analysis file, as it was not possible to save the
# result of conf_mat and keep the formatting pretty. Doing it this way ensures
# that it is formatted properly. Saving it as a csv led to the following error:
# Error in write_delim(x, file, delim = ",", na = na, append = append, col_names = col_names: is.data.frame(x) is not TRUE
# Converting it to a data.frame led to the table not being corretly formatted

confusion_matrix <- readRDS(here("results/analysis_data/confusion_matrix.rds"))
knitr::kable(confusion_matrix$table, caption = "Matrix for test data predictions")
```
From this confusion matrix, we can see that it predicted not fire correctly in 26 cases, predicted fire correctly in 33 cases and predicted incorrectly in only 1 case. This means that the accuracy of our model might be quite high. Now, we look at the accuracy metrics using the metrics argument:

```{r conclusion-plot, echo=FALSE, out.width="70%", fig.align="center", fig.cap = "All training data points and corresponding classification regions"}
knitr::include_graphics(here("results/analysis_data/classification_regions.png"))
```

From this graph, we can conclude that our model works fine because almost all the predictions are accurate. The colors don't seem to look patchy, which means that we didn't underfit this model. most of the observations were predicted correctly too.



# Discussion

## Result Summary
In this project, we made a model to predict whether or not there will be a forest fire based on the information given at the time of the observation. We found that we could in fact predict the presence of fire with high accuracy by using a K-nearest classification model and using ISI and BUI as our predictor variables.

This was what we expected to find because there had already been a strong relationship between ISI and BUI. Moreover, based on our preliminary data analysis we can already see that in both ISI and BUI, there is a somewhat clear distinction between values when there is or isn't any fire.

## Expectation and Impact
We hope that in the future, some machine learning models like this can be used to predict real fires, given a certain real-time environmental index. The prediction of forest fire will be significantly beneficial to society because according to such a model, people can strengthen the implementation of the precaution for forest fire when the predictors indicate an increased possibility of the fire, spread the fire alarm in advance, especially to the people who are living and working close to the dangerous zone and mobilize firefighters faster if necessary, etc. These all help limit the spread of forest fires efficiently so that many potential losses of lives and finance can be prevented to some degree.


## Limitations
The data set has limited our model set up to some degree, the reasons are as below:

- The number of records is limited. With only less than 250 observations, it is really hard to apply the model generated from our data set to the real-life scenario since we can not guarantee its accuracy even though our model shows a relabel accuracy in the result after the cross-validation. 

- The `Year` column shows all the records are valid in 2012 which has been passed for more than 10 years, so the prediction model we generated here would be more suitable to 'find a pattern in the past' instead of 'predict the foreseeable future forest fire'. So if we have our data set to be more up-to-date, the model could be closer to the real-life pattern nowadays and make more sense to the prediction.

Another limitation comes from the K-NN model itself. Given the nature that the K-NN method relies on calculating the distances among the observations, only the numerical variables can be used as the predictor. In fact, many categorical variables in our data set can also be significant predictors and contribute a lot to building the prediction model. 


## Further Questions
As mentioned above, the prediction model can effectively alert people when the possibility of a forest fire is detected. However, it can not specifically address the magnitude of any fire, so in the future study, we can also explore how to effectively build ML models to ‚Äúpredict the magnitude of a fire‚Äù.


# References
[1] Faroudja ABID et al. , Predicting Forest Fire in Algeria using Data Mining Techniques: Case Study of the Decision Tree Algorithms, International Conference on Advanced Intelligent Systems for Sustainable Development (AI2SD 2019) , 08 - 11 July , 2019, Marrakech, Morocco.

[2] Canadian Wildland Fire Information System | Canadian Forest Fire Weather Index (FWI) System, 2021.Retrieved 1 April 2021 from https://cwfis.cfs.nrcan.gc.ca/background/summary/fwi

[3] Fires in Algeria. (2021). Retrieved 1 April 2021, from https://earthobservatory.nasa.gov/images/18959/fires-in-algeria

[4] Template project used for reference in creation of this analysis: "https://github.com/UBC-DSCI/predict-airbnb-nightly-price/tree/v1.0.2"
