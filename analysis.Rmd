---
title: 'Predicting Forest Fires in Algeria'
author: "Rocky Zhao, Jinghan Xu, and Jan Zimny"
output:
  bookdown::html_document2:
    toc: true
  bookdown::pdf_document2:
    toc: true
    latex_engine: xelatex
---

```{r, include=FALSE}
library(here)
library(tidyverse)
library(tidymodels)
```

Forest fires are a type of uncontrolled and unwanted fire that usually have a negative impact. In 2007, forest fires in the Atlas Mountains, located on the northern coast of Algeria, killed several people as it spread rapidly due to hot, dry winds  [3].
In this project, the main question which we want to answer is:
- Can we predict forest fires given the weather conditions by using k nearest neighbours?
Our dataset contains weather information on the Sidi Bel Abb√®s and the Bejaia region, and includes information such as:

- `day`, `month`, `year`: the date of the instance

- `Temperature`: the maximum temperature in degrees celsius

- `RH`: Relative humidity in percentages

- `Ws`: wind speed in kilometers per hour

- `Rain`: total amount of rain, in mm

- `Classes`: whether or not there was fire

The data set also includes indexes:

- `FFMC`: Fine Fuel Moisture Code

- `DMC`: Dull Moisture Code

- `DC`: Drought Code

- `ISI`: Initial Spread Index

- `BUI`: Buildup Index

- `FWI`: Fire Weather Index

For more information on `FWI`, you can look at [this website](https://cwfis.cfs.nrcan.gc.ca/background/summary/fwi)

# Loading and wrangling the dataset
The first thing we do is to look at the dataset online, to see what it looks like. At first glance, we can see that this dataset contains headings, and uses commas as a delimiter. Moreover, it seems like each region is stored in a "separate" table on the same dataset. This means that we will need to do some cleaning after we load it to our notebook.
After inspecting the dataset, we take the data set's url from [this website](https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset++) and load it to our notebook. The dataset has one table for each region, so based on the loaded data we create 2 datasets.

```{r forest-fire-head, echo=FALSE, message=FALSE}
forest_fires <- read_csv(here("results/analysis_data/forest_fires.csv"))
knitr::kable(forest_fires[1:10, ],
  caption =
    "First 10 entries of the forest fire dataset"
)
```

# Exploring the dataset 

Usually, we do the preliminary exploratory data analysis with only the training set, so we need to split the `forest_fires` data set into a training and testing set first. Because our dataset is quite small, we decided to have more observations in the training set.|

```{r, echo = FALSE, message = FALSE}
fire_train <- read_csv(here("results/analysis_data/fire_train.csv"))
```

The training set takes up `r nrow(fire_train)/nrow(forest_fires)` percent of the total data.

After making the training set, now we can start exploring the data and decide what predictors to use. Because everything was recorded in 2012, so we don't want to use `day`, `month` and `year`. We removed these columns right before we split the data into the training and testing. Next, we want to try and summarize the data in different ways:

```{r entries-by-class, echo=FALSE, message=FALSE}
# this tells us how many entries are on each factor in Classes

fire_training_count <- read_csv(here("results/analysis_data/fire_training_count.csv"))
knitr::kable(fire_training_count, caption = "Entries by class")
```

From this, we know that there is a pretty equal distribution of observations between the two classes. This is a good thing, because it means we don't need any additional steps such as repeating the observations from the class with less observations.

```{r fire-training-range-fire-head, echo=FALSE, message=FALSE}
fire_training_range_fire <- read_csv(here("results/analysis_data/fire_training_range_fire.csv"))
knitr::kable(fire_training_range_fire, caption = "Value ranges of data samples which have the class 'fire'")
```

```{r fire-training-range-no-fire-head, echo=FALSE, message=FALSE}
fire_training_range_not_fire <- read_csv(here("results/analysis_data/fire_training_range_not_fire.csv"))
knitr::kable(fire_training_range_not_fire, caption = "Value ranges of data samples which have the class 'not fire'")
```

```{r fire-training-mean-fire-head, echo=FALSE, message=FALSE}
fire_training_mean_fire <- read_csv(here("results/analysis_data/fire_training_mean_fire.csv"))
knitr::kable(fire_training_mean_fire, caption = "Averages of data samples which have the class 'fire'")
```

```{r fire-training-mean-no-fire-head, echo=FALSE, message=FALSE}
fire_training_mean_not_fire <- read_csv(here("results/analysis_data/fire_training_mean_not_fire.csv"))
knitr::kable(fire_training_mean_not_fire, caption = "Averages of data samples which have the class 'not fire'")
```

We can see from table 2.2 and 2.3 that the ranges of values for Ws are really similar, which are 8-21 and 6-29. 
And from table 2.4 and 2.5, we can see that their averages are also really similar, which are 15.41 and 15.72.
This means that there isn't a clear distinction between the range of values from different Classes- and this also means that their averages are too similar. We don't want to use columns like this, because it's probably not going to be useful in differentiating the Classes. Another column we might not want to use isRH.
Because it's easier to process visual information, we draw some plots:

```{r correlation-plot, echo=FALSE, out.width="100%", fig.cap = "Correlations between each of the variables in the dataset"}
knitr::include_graphics(here("results/analysis_data/correlation_graph.png"))
```

As we can see from this plot, our previous conclusion that values like Ws won't be a good choice can be seen by how both rain and not rain Ws values overlap - we can't "group" the values into groups of rain/ not rain.
Based on prior research on `FWI` indexes [1], we know that they're all calculated based on temperature, relative humidity, wind and rain, which are the `Temperature`, `RH`, `Ws`, and `Rain` columns. Additionally, We also know that `ISI` is calculated from `Ws` and `FFMC`, and `BUI` is calculated from `DMC` and `DC`. These 2 columns encompass information from other columns, so it seems reasonable that we use them to predict `Classes`.
To confirm our choice, we make a plot with `ISI`, `BUI` and `Classes` to see if this choice makes sense based on the actual data itself.


```{r scatter-plot, echo=FALSE, out.width="70%", fig.align="center", fig.cap = "Scatter plot for variables ISI and BUI"}
knitr::include_graphics(here("results/analysis_data/scatter_plot.png"))
```

There is a positive correlation between `ISI` and `BUI`, and there seems to be a distinction between the Classes! So the columns `ISI` and `BUI` seem like a reasonable choice. Moreover, because we are only using 2 predictors, it'll be easier to make a visualization of our results/analysis_data.

Now that we know which predictors to use, we can start making our model.


# Performing the data analysis

Because we are trying to predict `Classes`, which is a categorical variable, with `ISI` AND `BUI`, which are quantitative variables, this is a classification problem. So, we can make a k-nearest classification model, using the functions from tidymodels to help us. Before we can make the model, we need to choose a _k_ value that works. So, we need to tune the classifier. 
The steps are as follows:

1. We make a 10-fold cross validation, to generate the testing and validation sets. Because our dataset is relatively small, we decided to use 10-fold instead of 5 fold to compensate for the fewer observations.

2. Make the recipe, using Classes as the target variable and `ISI` and `BUI` as the predictor variables, using data from the training set.

3. Then, we make a model specification with the `nearest_neighbor` function, setting the neighbors to `tune()`.

4. We add this recipe and model to a `workflow()`, using `collect_metrics()` to get the accuracies of each _k_ value.


```{r accuracies-head, echo=FALSE, message=FALSE}
accuracies <- read_csv(here("results/analysis_data/accuracies.csv"))
knitr::kable(accuracies[1:10, ], caption = "Accuracies per number of neighbors respected, first ten rows of data")
```

Now, we know how accurate our model is for different _k_ values- but since we have 50 values, it's hard to compare all 50 at once! Again, We plot the accuracies so that we can compare the accuracies between each _k_ value in a more efficient and better way.

```{r line-plot, echo=FALSE, out.width="70%", fig.align="center", fig.cap = "Line plot visualizing the accuracy per number of neighbors taken into account while classifying"}
knitr::include_graphics(here("results/analysis_data/line_plot.png"))
```

From this graph, we can see the it peaks at about 5/6 and then starts to reduce as _k_ increases. K-nearest model gets slower with larger _k_ values, so we decided that k = 5 would be the most ideal choice for our model, because of its high accuracy estimate and since this estimate doesn't change much when compared to nearby k values.
This graph also makes sense because we know that small K values tend to have lower accuracies due to underfitting, while large k values will also have lower accuracies due to overfitting. In this graph, accuracies initially increase, and then generally decreases as _k_ values increase.
Now that we know what _k_ value to use, we make a new model specification, this time setting _k_ to 5. We add this new model and the recipe to a workflow.

Now, that we have a model, we can use it to predict the classes on our testing set!


```{r fire-pred-head, echo=FALSE, message=FALSE}
fire_test_predictions <- read_csv(here("results/analysis_data/fire_test_predictions.csv"))
knitr::kable(fire_test_predictions[1:10, ], caption = "Peak into the entries which have been classified, first ten rows of data")
```

To get a better picture on how well our model predicted the testing set's classes, we can get the metrics and the confusion matrix:

```{r, echo=FALSE}
# We kept this code in the analysis file, as it was not possible to save the
# result of conf_mat and keep the formatting pretty. Doing it this way ensures
# that it is formatted properly. Saving it as a csv led to the following error:
# Error in write_delim(x, file, delim = ",", na = na, append = append, col_names = col_names: is.data.frame(x) is not TRUE
# Converting it to a data.frame led to the table not being corretly formatted

confusion_matrix <- readRDS(here("results/analysis_data/confusion_matrix.rds"))
knitr::kable(confusion_matrix$table, caption = "Matrix for test data predictions")
```
From this confusion matrix, we can see that it predicted not fire correctly in 25 cases, predicted fire correctly in 32 cases and predicted incorrectly in only 3 cases. This means that the accuracy of our model might be quite high. Now, we look at the accuracy metrics using the metrics argument:

```{r conclusion-plot, echo=FALSE, out.width="70%", fig.align="center", fig.cap = "All training data points and corresponding classification regions"}
knitr::include_graphics(here("results/analysis_data/classification_regions.png"))
```

From this graph, we can conclude that our model works fine - almost all the predictions are accurate. The colors don't seem to look patchy, which means that we didn't underfit this model. most of the observations were predicted correctly too.


# Wrapping it all up

In this project, we made a model to predict whether or not there will be a forest fire based on information given at the time of the observation. We found that we could in fact predict the presence of fire with a high accuracy by using a k-nearest classification model and using ISI and BUI as our predictor variables.
This was what we expected to find, because there had already been a strong relationship between ISI and BUI. Moreover, based on our preliminary data analysis we can already see that in both ISI and BUI, there is a somewhat clear distinction between values when there is or isn't any fire.

**Expected outcomes and significance:**

We hope that in the future, something like this can be used to predict real fires. Predicting fire will be beneficial because it means we can mobilise firefighters faster, which helps stop the spread of fires. Hopefully, this can also lead to other questions, such as predicting the magnitude of a fire.


# References
[1] Faroudja ABID et al. , Predicting Forest Fire in Algeria using Data Mining Techniques: Case Study of the Decision Tree Algorithms, International Conference on Advanced Intelligent Systems for Sustainable Development (AI2SD 2019) , 08 - 11 July , 2019, Marrakech, Morocco.

[2] Canadian Wildland Fire Information System | Canadian Forest Fire Weather Index (FWI) System, 2021.Retrieved 1 April 2021 from https://cwfis.cfs.nrcan.gc.ca/background/summary/fwi

[3] Fires in Algeria. (2021). Retrieved 1 April 2021, from https://earthobservatory.nasa.gov/images/18959/fires-in-algeria

[4] Template project used for reference in creation of this analysis: "https://github.com/UBC-DSCI/predict-airbnb-nightly-price/tree/v1.0.2"
